{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72ded19e",
   "metadata": {},
   "source": [
    "<H1>Linear Regression with One Independent Variable</H1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f033b5f",
   "metadata": {},
   "source": [
    "In this exercise, we are going to load some (x, y) data and perform a curve fitting. As there is one independent and one dependent variable, the linear regression equation fitting such data points should be of the form of $y = mx + c$ where m is the slope and c is the y-intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f96a69c",
   "metadata": {},
   "source": [
    "As the first step, let us load some useful packages and the data points. As always, visualization is a great step to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61052ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (8.0, 5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4286bf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Input data\n",
    "data = pd.read_csv('wk2_data.csv')\n",
    "X = data.iloc[:, 0]\n",
    "Y = data.iloc[:, 1]\n",
    "plt.scatter(X, Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce78a097",
   "metadata": {},
   "source": [
    "We are going to solve the problem using two different approaches. The first approach is to compute the optimal m and c mathematically. The second is using gradient descent.\n",
    "\n",
    "To start, we need to define the optimization objective. Let us set the objective to be minimizing the sum of squares of the error\n",
    "\n",
    "\\\\[ L(x) = \\sum_{i=1}^n (y_i - p_i)^2\\\\] \n",
    "\n",
    "where $y_i$ is the truth value associated with the $i^{th}$ sample and $p_i$ is the associated prediction using $y = mx + c$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b04592b",
   "metadata": {},
   "source": [
    "<H2>Find the optimal solution analytically</H2>\n",
    "\n",
    "To find the solution analytically, we need to <i>minimize</i> $L(x)$. Could you figure out the equation for $m$ and $c$? (Hint: using calculus to find the coefficients minimizing $L(x)$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22f824d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the model\n",
    "X_mean = np.mean(X)\n",
    "Y_mean = np.mean(Y)\n",
    "\n",
    "c = # your code here\n",
    "\n",
    "num = 0\n",
    "den = 0\n",
    "for i in range(len(X)):\n",
    "    num += # your code here\n",
    "    den += # your code here\n",
    "m = num / den\n",
    "\n",
    "\n",
    "print (m, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85136bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions\n",
    "Y_pred = m*X + c\n",
    "\n",
    "plt.scatter(X, Y) # actual\n",
    "plt.plot([min(X), max(X)], [min(Y_pred), max(Y_pred)], color='red') # predicted\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487da3a2",
   "metadata": {},
   "source": [
    "<H3>Discussions:</H3>\n",
    "\n",
    "<li>Why do we need alternative methods?</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a90998",
   "metadata": {},
   "source": [
    "<H2>Finding an optimal solution using gradient descent</H2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b682c26e",
   "metadata": {},
   "source": [
    "Gradient descent counts on updating $m$ and $c$ iteratively. Could you figure out the the equations updating both $m$ and $c$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03eeae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the model\n",
    "m = 0\n",
    "c = 0\n",
    "\n",
    "L = 0.00001  # The learning Rate\n",
    "epochs = 1000  # The number of iterations to perform gradient descent\n",
    "\n",
    "n = float(len(X)) # Number of elements in X\n",
    "\n",
    "# Performing Gradient Descent \n",
    "for i in range(epochs):     \n",
    "    clear_output(wait=True)\n",
    "    plt.show()\n",
    "    Y_pred = m*X + c  # The current predicted value of Y\n",
    "    D_m = # Your code here - Derivative wrt m\n",
    "    D_c = (-2# Derivative wrt c\n",
    "    m = m - L * D_m  # Update m\n",
    "    c = c - L * D_c  # Update c\n",
    "    plt.scatter(X, Y)\n",
    "    plt.plot([min(X), max(X)], [min(Y_pred), max(Y_pred)], color='red') # predicted\n",
    "    plt.show()   \n",
    "    #time.sleep(.5)\n",
    "print (m, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92ab44c",
   "metadata": {},
   "source": [
    "<H3>Discussions:</H3>\n",
    "    <li>Do the answer match with the analytical answer? \n",
    "    <li>Why or why not? If not, could you make it match?\n",
    "    <li>How about the learning rate? Please try incrase it by 10X and reduce it by 10X. What do you observe?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
